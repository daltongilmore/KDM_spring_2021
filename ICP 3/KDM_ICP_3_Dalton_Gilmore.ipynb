{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KDM ICP 3 Dalton Gilmore",
      "provenance": [],
      "collapsed_sections": [
        "oVWrAG0jOTkd",
        "shcacXfo3K8s",
        "1vzygC2D3PLT",
        "5RPSx4OuFDKa",
        "5ZvAJdHmFJGO",
        "q44_TdMpFJ67",
        "awYWpCw0FKAm",
        "gVWD_zqbFKGH"
      ],
      "authorship_tag": "ABX9TyOBw+aXozgILieggnGosiyW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daltongilmore/KDM_spring_2021/blob/master/KDM_ICP_3_Dalton_Gilmore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVWrAG0jOTkd"
      },
      "source": [
        "# Initial Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IlStPunS2ib"
      },
      "source": [
        "Import the necessary libraries\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxbqiWaOs8OY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c94ca04-8bf5-4fb0-e775-ad7add817580"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('all')\r\n",
        "from nltk.corpus import wordnet\r\n",
        "!pip install spacy\r\n",
        "!python3 -m spacy download en_core_web_sm\r\n",
        "import spacy"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (53.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (53.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shcacXfo3K8s"
      },
      "source": [
        "# Triplet Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIFZ1JDs2_Ge"
      },
      "source": [
        "Create input sentences and put them in a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHJ3aq5FzLpX"
      },
      "source": [
        "text1 = \"Ketchup belongs on every burger.\" \r\n",
        "text2 = \"My girlfriend loves me very much.\"\r\n",
        "text3 = \"Squirrels, also known as Sciuridae, eat a lot of nuts.\"\r\n",
        "text4 = \"Google often merges with many other applications.\"\r\n",
        "text5 = \"My professor bought a bunch of laptops last Tuesday.\""
      ],
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q1U312P54Hk"
      },
      "source": [
        "SentList = []\r\n",
        "SentList.append(text1)\r\n",
        "SentList.append(text2)\r\n",
        "SentList.append(text3)\r\n",
        "SentList.append(text4)\r\n",
        "SentList.append(text5)"
      ],
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwu-t6fQ3dJA"
      },
      "source": [
        "Use spaCy to show dependencies and parts of speech for each word in a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUfGjkYJwc-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c6b3d8-d116-4538-a77c-d3d43c6d7908"
      },
      "source": [
        "nlp = spacy.load('en')\r\n",
        "doc = nlp(text3) # Any sentence can go here - this is just for understanding the extraction\r\n",
        "\r\n",
        "for tok in doc: \r\n",
        "  print(tok.text,\">>>\",tok.dep_,\">>>\",tok.pos_)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Squirrels >>> nsubj >>> NOUN\n",
            ", >>> punct >>> PUNCT\n",
            "also >>> advmod >>> ADV\n",
            "known >>> acl >>> VERB\n",
            "as >>> prep >>> SCONJ\n",
            "Sciuridae >>> pobj >>> PROPN\n",
            ", >>> punct >>> PUNCT\n",
            "eat >>> ROOT >>> VERB\n",
            "a >>> det >>> DET\n",
            "lot >>> dobj >>> NOUN\n",
            "of >>> prep >>> ADP\n",
            "nuts >>> pobj >>> NOUN\n",
            ". >>> punct >>> PUNCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjcEmx_Y4hu8"
      },
      "source": [
        "Function for triplet extraction utilizing dependency tags and matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1F2jMPcw6y6"
      },
      "source": [
        "def triplet(doc):\r\n",
        "  subjpass = 0\r\n",
        "\r\n",
        "  for i,tok in enumerate(doc):\r\n",
        "    # Find out if sentence is passive or not by looking for \"subjpass\"  \r\n",
        "    if tok.dep_.find(\"subjpass\") == True:\r\n",
        "      subjpass = 1\r\n",
        "\r\n",
        "  x = ''\r\n",
        "  y = ''\r\n",
        "  z = ''\r\n",
        "\r\n",
        "  # If sentence is passive\r\n",
        "  if subjpass == 1:\r\n",
        "    for i,tok in enumerate(doc):\r\n",
        "\r\n",
        "      if tok.dep_.endswith(\"obj\") == True:\r\n",
        "        x = tok.text\r\n",
        "\r\n",
        "      if tok.dep_.endswith(\"ROOT\") == True:\r\n",
        "        y = tok.text\r\n",
        "\r\n",
        "      if tok.dep_.find(\"subjpass\") == True:\r\n",
        "        z = tok.text\r\n",
        "\r\n",
        "  # If sentence is not passive\r\n",
        "  else:\r\n",
        "    for i,tok in enumerate(doc):\r\n",
        "\r\n",
        "      if tok.dep_.endswith(\"subj\") == True:\r\n",
        "        x = tok.text\r\n",
        "\r\n",
        "      if tok.dep_.endswith(\"ROOT\") == True:\r\n",
        "        y = tok.text\r\n",
        "\r\n",
        "      if tok.dep_.endswith(\"obj\") == True:\r\n",
        "        z = tok.text\r\n",
        "\r\n",
        "  return x,y,z # Return object, subject, and root verb that refers to both"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy2svdJD6xjS"
      },
      "source": [
        "Use the function on each sentence in order to achieve triplet extraction in the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3euHqJcvxlk8",
        "outputId": "8098209f-868d-4585-f2ca-85e20d24c6e7"
      },
      "source": [
        "for i in SentList:\r\n",
        "  print(triplet(nlp(i)),'\\n')"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Ketchup', 'belongs', 'burger') \n",
            "\n",
            "('girlfriend', 'loves', 'me') \n",
            "\n",
            "('Squirrels', 'eat', 'nuts') \n",
            "\n",
            "('Google', 'merges', 'applications') \n",
            "\n",
            "('professor', 'bought', 'laptops') \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vzygC2D3PLT"
      },
      "source": [
        "# WordNet Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RPSx4OuFDKa"
      },
      "source": [
        "## 1. Hyponym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlbitONCFjkH"
      },
      "source": [
        "Define function to get all hyponyms of a synset using union of sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkeOcM1o-TN4"
      },
      "source": [
        "def get_hyponyms(synset):\r\n",
        "  hyponyms = set()\r\n",
        "  for hyponym in synset.hyponyms():\r\n",
        "    hyponyms |= set(get_hyponyms(hyponym))\r\n",
        "  return hyponyms | set(synset.hyponyms())"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moHEOj-QFlh0"
      },
      "source": [
        "Retrieve the synset of a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyJJTDm5E_w4"
      },
      "source": [
        "syn = wordnet.synsets('fruit')[0]"
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKHRvLUSFp5y"
      },
      "source": [
        "Use the function to retrieve all hyponyms of the word in a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl1t0UEREnLl",
        "outputId": "a18e0891-f673-417d-d7a3-5a1d3c6d3ecb"
      },
      "source": [
        "get_hyponyms(syn)"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Synset('accessory_fruit.n.01'),\n",
              " Synset('acerola.n.02'),\n",
              " Synset('achene.n.01'),\n",
              " Synset('acinus.n.01'),\n",
              " Synset('ackee.n.01'),\n",
              " Synset('acorn.n.01'),\n",
              " Synset('aggregate_fruit.n.01'),\n",
              " Synset('algarroba.n.03'),\n",
              " Synset('almond.n.02'),\n",
              " Synset('amaranth.n.01'),\n",
              " Synset('amarelle.n.02'),\n",
              " Synset('anchovy_pear.n.02'),\n",
              " Synset('anjou.n.02'),\n",
              " Synset('apple.n.01'),\n",
              " Synset('apricot.n.02'),\n",
              " Synset('ash-key.n.01'),\n",
              " Synset('avocado.n.01'),\n",
              " Synset('babassu_nut.n.01'),\n",
              " Synset('baldwin.n.03'),\n",
              " Synset('banana.n.02'),\n",
              " Synset('baneberry.n.01'),\n",
              " Synset('barbados_gooseberry.n.02'),\n",
              " Synset('barleycorn.n.01'),\n",
              " Synset('bartlett.n.03'),\n",
              " Synset('beach_plum.n.02'),\n",
              " Synset('bean.n.02'),\n",
              " Synset('beechnut.n.01'),\n",
              " Synset('bell_apple.n.01'),\n",
              " Synset('berry.n.01'),\n",
              " Synset('berry.n.02'),\n",
              " Synset('betel_nut.n.01'),\n",
              " Synset('bilberry.n.03'),\n",
              " Synset('bing_cherry.n.01'),\n",
              " Synset('bitter_orange.n.02'),\n",
              " Synset('black_currant.n.02'),\n",
              " Synset('black_olive.n.01'),\n",
              " Synset('black_walnut.n.02'),\n",
              " Synset('blackberry.n.01'),\n",
              " Synset('blackheart.n.02'),\n",
              " Synset('blueberry.n.02'),\n",
              " Synset('bonduc_nut.n.01'),\n",
              " Synset('bosc.n.01'),\n",
              " Synset('boysenberry.n.02'),\n",
              " Synset('bramley's_seedling.n.01'),\n",
              " Synset('brazil_nut.n.02'),\n",
              " Synset('breadfruit.n.02'),\n",
              " Synset('broad_bean.n.01'),\n",
              " Synset('buckeye.n.01'),\n",
              " Synset('buckthorn_berry.n.01'),\n",
              " Synset('buffalo_nut.n.01'),\n",
              " Synset('bullock's_heart.n.02'),\n",
              " Synset('bunya_bunya.n.02'),\n",
              " Synset('butternut.n.02'),\n",
              " Synset('calabar_bean.n.01'),\n",
              " Synset('calabash.n.01'),\n",
              " Synset('candlenut.n.02'),\n",
              " Synset('canistel.n.02'),\n",
              " Synset('cantaloup.n.02'),\n",
              " Synset('capulin.n.02'),\n",
              " Synset('carambola.n.02'),\n",
              " Synset('carissa_plum.n.01'),\n",
              " Synset('carob.n.01'),\n",
              " Synset('casaba.n.01'),\n",
              " Synset('cashew.n.02'),\n",
              " Synset('castor_bean.n.01'),\n",
              " Synset('catawba.n.02'),\n",
              " Synset('cembra_nut.n.01'),\n",
              " Synset('ceriman.n.02'),\n",
              " Synset('cherimoya.n.02'),\n",
              " Synset('cherry.n.03'),\n",
              " Synset('chestnut.n.03'),\n",
              " Synset('chickpea.n.01'),\n",
              " Synset('chincapin.n.01'),\n",
              " Synset('chokecherry.n.01'),\n",
              " Synset('citrange.n.02'),\n",
              " Synset('citron.n.01'),\n",
              " Synset('citrus.n.01'),\n",
              " Synset('clementine.n.02'),\n",
              " Synset('cling.n.01'),\n",
              " Synset('cocoa_plum.n.02'),\n",
              " Synset('coconut.n.02'),\n",
              " Synset('coffee_bean.n.01'),\n",
              " Synset('cohune_nut.n.01'),\n",
              " Synset('concord_grape.n.01'),\n",
              " Synset('cooking_apple.n.01'),\n",
              " Synset('coquilla_nut.n.01'),\n",
              " Synset('corn.n.02'),\n",
              " Synset('cortland.n.01'),\n",
              " Synset('cottonseed.n.01'),\n",
              " Synset('cowage.n.01'),\n",
              " Synset('cowpea.n.01'),\n",
              " Synset('cox's_orange_pippin.n.01'),\n",
              " Synset('crab_apple.n.03'),\n",
              " Synset('cranberry.n.02'),\n",
              " Synset('cubeb.n.01'),\n",
              " Synset('cumin.n.02'),\n",
              " Synset('currant.n.01'),\n",
              " Synset('currant.n.03'),\n",
              " Synset('custard_apple.n.02'),\n",
              " Synset('damson.n.01'),\n",
              " Synset('date.n.08'),\n",
              " Synset('delicious.n.01'),\n",
              " Synset('dewberry.n.02'),\n",
              " Synset('dika_nut.n.01'),\n",
              " Synset('divi-divi.n.01'),\n",
              " Synset('dried_apricot.n.01'),\n",
              " Synset('dried_fruit.n.01'),\n",
              " Synset('drupe.n.01'),\n",
              " Synset('drupelet.n.01'),\n",
              " Synset('durian.n.02'),\n",
              " Synset('ear.n.05'),\n",
              " Synset('eating_apple.n.01'),\n",
              " Synset('edible_fruit.n.01'),\n",
              " Synset('edible_nut.n.01'),\n",
              " Synset('edible_seed.n.01'),\n",
              " Synset('elderberry.n.02'),\n",
              " Synset('emperor.n.02'),\n",
              " Synset('empire.n.05'),\n",
              " Synset('english_walnut.n.02'),\n",
              " Synset('feijoa.n.02'),\n",
              " Synset('field_pea.n.01'),\n",
              " Synset('fig.n.04'),\n",
              " Synset('flame_tokay.n.01'),\n",
              " Synset('fox_grape.n.02'),\n",
              " Synset('freestone.n.01'),\n",
              " Synset('fruitlet.n.01'),\n",
              " Synset('garambulla.n.02'),\n",
              " Synset('garden_pea.n.01'),\n",
              " Synset('genip.n.02'),\n",
              " Synset('genipap.n.01'),\n",
              " Synset('golden_delicious.n.01'),\n",
              " Synset('gooseberry.n.02'),\n",
              " Synset('gourd.n.02'),\n",
              " Synset('grain.n.07'),\n",
              " Synset('granadilla.n.04'),\n",
              " Synset('granny_smith.n.01'),\n",
              " Synset('grape.n.01'),\n",
              " Synset('grapefruit.n.02'),\n",
              " Synset('green_olive.n.01'),\n",
              " Synset('greengage.n.01'),\n",
              " Synset('grimes'_golden.n.01'),\n",
              " Synset('grugru_nut.n.01'),\n",
              " Synset('guava.n.03'),\n",
              " Synset('hagberry.n.01'),\n",
              " Synset('hazelnut.n.02'),\n",
              " Synset('heart_cherry.n.02'),\n",
              " Synset('hickory_nut.n.01'),\n",
              " Synset('hip.n.05'),\n",
              " Synset('hog_plum.n.03'),\n",
              " Synset('hog_plum.n.04'),\n",
              " Synset('honeydew.n.01'),\n",
              " Synset('huckleberry.n.03'),\n",
              " Synset('ilama.n.02'),\n",
              " Synset('ivory_nut.n.01'),\n",
              " Synset('jaboticaba.n.02'),\n",
              " Synset('jackfruit.n.02'),\n",
              " Synset('jaffa_orange.n.01'),\n",
              " Synset('job's_tears.n.01'),\n",
              " Synset('jonathan.n.01'),\n",
              " Synset('jordan_almond.n.02'),\n",
              " Synset('jujube.n.02'),\n",
              " Synset('jumping_bean.n.01'),\n",
              " Synset('juniper_berry.n.01'),\n",
              " Synset('kai_apple.n.01'),\n",
              " Synset('kernel.n.02'),\n",
              " Synset('ketembilla.n.02'),\n",
              " Synset('key_lime.n.01'),\n",
              " Synset('kiwi.n.03'),\n",
              " Synset('kola_nut.n.01'),\n",
              " Synset('kumquat.n.02'),\n",
              " Synset('lane's_prince_albert.n.01'),\n",
              " Synset('lanseh.n.01'),\n",
              " Synset('legume.n.02'),\n",
              " Synset('lemon.n.01'),\n",
              " Synset('lentil.n.02'),\n",
              " Synset('lime.n.06'),\n",
              " Synset('lingonberry.n.02'),\n",
              " Synset('linseed.n.01'),\n",
              " Synset('litchi.n.02'),\n",
              " Synset('loganberry.n.02'),\n",
              " Synset('loment.n.01'),\n",
              " Synset('longanberry.n.02'),\n",
              " Synset('loquat.n.02'),\n",
              " Synset('macadamia_nut.n.02'),\n",
              " Synset('macoun.n.01'),\n",
              " Synset('mamey.n.02'),\n",
              " Synset('mandarin.n.05'),\n",
              " Synset('mango.n.02'),\n",
              " Synset('mangosteen.n.02'),\n",
              " Synset('marang.n.02'),\n",
              " Synset('marasca.n.01'),\n",
              " Synset('mast.n.02'),\n",
              " Synset('may_apple.n.01'),\n",
              " Synset('mcintosh.n.01'),\n",
              " Synset('mealie.n.01'),\n",
              " Synset('medlar.n.03'),\n",
              " Synset('medlar.n.04'),\n",
              " Synset('melon.n.01'),\n",
              " Synset('melon_ball.n.01'),\n",
              " Synset('mombin.n.02'),\n",
              " Synset('morello.n.02'),\n",
              " Synset('mulberry.n.02'),\n",
              " Synset('muscadine.n.02'),\n",
              " Synset('muscat.n.04'),\n",
              " Synset('muskmelon.n.02'),\n",
              " Synset('navel_orange.n.01'),\n",
              " Synset('nectarine.n.02'),\n",
              " Synset('neem_seed.n.01'),\n",
              " Synset('net_melon.n.02'),\n",
              " Synset('newtown_wonder.n.01'),\n",
              " Synset('northern_spy.n.01'),\n",
              " Synset('nut.n.01'),\n",
              " Synset('nutlet.n.01'),\n",
              " Synset('oilseed.n.01'),\n",
              " Synset('okra.n.01'),\n",
              " Synset('olive.n.01'),\n",
              " Synset('olive.n.04'),\n",
              " Synset('orange.n.01'),\n",
              " Synset('palm_nut.n.01'),\n",
              " Synset('papaw.n.02'),\n",
              " Synset('papaya.n.02'),\n",
              " Synset('passion_fruit.n.01'),\n",
              " Synset('pea.n.02'),\n",
              " Synset('peach.n.03'),\n",
              " Synset('peanut.n.01'),\n",
              " Synset('peanut.n.04'),\n",
              " Synset('pear.n.01'),\n",
              " Synset('pearmain.n.01'),\n",
              " Synset('pecan.n.03'),\n",
              " Synset('persian_melon.n.02'),\n",
              " Synset('persimmon.n.02'),\n",
              " Synset('pine_nut.n.01'),\n",
              " Synset('pineapple.n.02'),\n",
              " Synset('pip.n.03'),\n",
              " Synset('pippin.n.01'),\n",
              " Synset('pistachio.n.02'),\n",
              " Synset('pitahaya.n.02'),\n",
              " Synset('plum.n.02'),\n",
              " Synset('plumcot.n.02'),\n",
              " Synset('pod.n.02'),\n",
              " Synset('pome.n.01'),\n",
              " Synset('pomegranate.n.02'),\n",
              " Synset('pomelo.n.02'),\n",
              " Synset('pond_apple.n.02'),\n",
              " Synset('prairie_gourd.n.01'),\n",
              " Synset('prickly_pear.n.02'),\n",
              " Synset('prima.n.01'),\n",
              " Synset('prune.n.01'),\n",
              " Synset('pulasan.n.02'),\n",
              " Synset('pumpkin_seed.n.01'),\n",
              " Synset('pyrene.n.02'),\n",
              " Synset('pyxidium.n.01'),\n",
              " Synset('quandong.n.02'),\n",
              " Synset('quandong.n.04'),\n",
              " Synset('quandong_nut.n.01'),\n",
              " Synset('quince.n.02'),\n",
              " Synset('raisin.n.01'),\n",
              " Synset('rambutan.n.02'),\n",
              " Synset('rapeseed.n.01'),\n",
              " Synset('raspberry.n.02'),\n",
              " Synset('red_currant.n.02'),\n",
              " Synset('red_delicious.n.01'),\n",
              " Synset('ribier.n.01'),\n",
              " Synset('rome_beauty.n.01'),\n",
              " Synset('rose_apple.n.02'),\n",
              " Synset('rowanberry.n.01'),\n",
              " Synset('rye.n.01'),\n",
              " Synset('safflower_seed.n.01'),\n",
              " Synset('samara.n.01'),\n",
              " Synset('sapodilla.n.02'),\n",
              " Synset('sapote.n.02'),\n",
              " Synset('saskatoon.n.02'),\n",
              " Synset('satsuma.n.02'),\n",
              " Synset('schizocarp.n.01'),\n",
              " Synset('screw_bean.n.01'),\n",
              " Synset('scuppernong.n.01'),\n",
              " Synset('seckel.n.01'),\n",
              " Synset('seed.n.01'),\n",
              " Synset('seeded_raisin.n.01'),\n",
              " Synset('seedless_raisin.n.01'),\n",
              " Synset('simple_fruit.n.01'),\n",
              " Synset('slipskin_grape.n.01'),\n",
              " Synset('sloe.n.03'),\n",
              " Synset('sorb.n.01'),\n",
              " Synset('souari_nut.n.02'),\n",
              " Synset('sour_cherry.n.03'),\n",
              " Synset('sour_gourd.n.02'),\n",
              " Synset('sour_gourd.n.03'),\n",
              " Synset('soursop.n.02'),\n",
              " Synset('soy.n.01'),\n",
              " Synset('stayman.n.01'),\n",
              " Synset('stayman_winesap.n.01'),\n",
              " Synset('strawberry.n.01'),\n",
              " Synset('sugarberry.n.02'),\n",
              " Synset('sultana.n.01'),\n",
              " Synset('sunflower_seed.n.01'),\n",
              " Synset('sweet_calabash.n.02'),\n",
              " Synset('sweet_cherry.n.02'),\n",
              " Synset('sweet_orange.n.01'),\n",
              " Synset('sweetsop.n.02'),\n",
              " Synset('syconium.n.01'),\n",
              " Synset('tamarind.n.02'),\n",
              " Synset('tangelo.n.02'),\n",
              " Synset('tangerine.n.02'),\n",
              " Synset('temple_orange.n.02'),\n",
              " Synset('thompson_seedless.n.01'),\n",
              " Synset('tokay.n.02'),\n",
              " Synset('tonka_bean.n.01'),\n",
              " Synset('valencia_orange.n.01'),\n",
              " Synset('victoria_plum.n.01'),\n",
              " Synset('vinifera_grape.n.02'),\n",
              " Synset('walnut.n.01'),\n",
              " Synset('water_chinquapin.n.02'),\n",
              " Synset('watermelon.n.02'),\n",
              " Synset('wheat_berry.n.01'),\n",
              " Synset('wild_cherry.n.01'),\n",
              " Synset('windfall.n.01'),\n",
              " Synset('winesap.n.01'),\n",
              " Synset('winter_melon.n.02'),\n",
              " Synset('wintergreen.n.03')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZvAJdHmFJGO"
      },
      "source": [
        "## 2. Hypernym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKxZpF7nGD9p"
      },
      "source": [
        "Define function to get all hypernyms of a synset using union of sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv1WyYX1GD9r"
      },
      "source": [
        "def get_hypernyms(synset):\r\n",
        "  hypernyms = set()\r\n",
        "  for hypernym in synset.hypernyms():\r\n",
        "    hypernyms |= set(get_hypernyms(hypernym))\r\n",
        "  return hypernyms | set(synset.hypernyms())"
      ],
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nLvXRpnGD9s"
      },
      "source": [
        "Retrieve the synset of a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1Jz2HH7GD9s"
      },
      "source": [
        "syn = wordnet.synsets('orange')[0]"
      ],
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJbGYNmXGD9s"
      },
      "source": [
        "Use the function to retrieve all hypernyms of the word in a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlhMe2MpGD9s",
        "outputId": "e3024da5-e7c4-4f44-e1b6-8f59ce2c834a"
      },
      "source": [
        "get_hypernyms(syn)"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Synset('citrus.n.01'),\n",
              " Synset('edible_fruit.n.01'),\n",
              " Synset('entity.n.01'),\n",
              " Synset('food.n.02'),\n",
              " Synset('fruit.n.01'),\n",
              " Synset('matter.n.03'),\n",
              " Synset('natural_object.n.01'),\n",
              " Synset('object.n.01'),\n",
              " Synset('physical_entity.n.01'),\n",
              " Synset('plant_organ.n.01'),\n",
              " Synset('plant_part.n.01'),\n",
              " Synset('produce.n.01'),\n",
              " Synset('reproductive_structure.n.01'),\n",
              " Synset('solid.n.01'),\n",
              " Synset('whole.n.02')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q44_TdMpFJ67"
      },
      "source": [
        "## 3. Meronym "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVtkIk1pGueS"
      },
      "source": [
        "Define function to get all meronyms of a synset using union of sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVgpMnWtGueT"
      },
      "source": [
        "def get_meronyms(synset):\r\n",
        "  meronyms = set()\r\n",
        "  for meronym in synset.part_meronyms():\r\n",
        "    meronyms |= set(get_meronyms(meronym))\r\n",
        "  return meronyms | set(synset.part_meronyms())"
      ],
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp2BO9BZGueU"
      },
      "source": [
        "Retrieve the synset of a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbc0TcRMGueU"
      },
      "source": [
        "syn = wordnet.synsets('car')[0]"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00Nx6C6wGueU"
      },
      "source": [
        "Use the function to retrieve all meronyms of the word in a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83ht3GxtGueU",
        "outputId": "d9689998-bfc8-4438-a6b4-7a1a3d18279d"
      },
      "source": [
        "get_meronyms(syn)"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Synset('accelerator.n.01'),\n",
              " Synset('air_bag.n.01'),\n",
              " Synset('armrest.n.01'),\n",
              " Synset('auto_accessory.n.01'),\n",
              " Synset('automobile_engine.n.01'),\n",
              " Synset('automobile_horn.n.01'),\n",
              " Synset('back.n.08'),\n",
              " Synset('buffer.n.06'),\n",
              " Synset('bumper.n.02'),\n",
              " Synset('bumper_guard.n.01'),\n",
              " Synset('car_door.n.01'),\n",
              " Synset('car_mirror.n.01'),\n",
              " Synset('car_seat.n.01'),\n",
              " Synset('car_window.n.01'),\n",
              " Synset('doorlock.n.01'),\n",
              " Synset('exhaust.n.02'),\n",
              " Synset('exhaust_manifold.n.01'),\n",
              " Synset('exhaust_pipe.n.01'),\n",
              " Synset('exhaust_valve.n.01'),\n",
              " Synset('fender.n.01'),\n",
              " Synset('first_gear.n.01'),\n",
              " Synset('floorboard.n.02'),\n",
              " Synset('gasoline_engine.n.01'),\n",
              " Synset('glove_compartment.n.01'),\n",
              " Synset('grille.n.02'),\n",
              " Synset('headrest.n.01'),\n",
              " Synset('high_gear.n.01'),\n",
              " Synset('hinge.n.01'),\n",
              " Synset('hood.n.09'),\n",
              " Synset('hood_ornament.n.01'),\n",
              " Synset('horn_button.n.01'),\n",
              " Synset('inlet_manifold.n.01'),\n",
              " Synset('luggage_compartment.n.01'),\n",
              " Synset('pintle.n.01'),\n",
              " Synset('rear_window.n.01'),\n",
              " Synset('reverse.n.02'),\n",
              " Synset('roof.n.02'),\n",
              " Synset('running_board.n.01'),\n",
              " Synset('seat_belt.n.01'),\n",
              " Synset('silencer.n.02'),\n",
              " Synset('stabilizer_bar.n.01'),\n",
              " Synset('sunroof.n.01'),\n",
              " Synset('tail_fin.n.02'),\n",
              " Synset('tailpipe.n.01'),\n",
              " Synset('third_gear.n.01'),\n",
              " Synset('window.n.02')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awYWpCw0FKAm"
      },
      "source": [
        "## 4. Holonym "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRyfwR6iJ-Qy"
      },
      "source": [
        "Define function to get all holonyms of a synset using union of sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFNd_MkGJ-Q0"
      },
      "source": [
        "def get_holonyms(synset):\r\n",
        "  holonyms = set()\r\n",
        "  for holonym in synset.part_holonyms():\r\n",
        "    holonyms |= set(get_holonyms(holonym))\r\n",
        "  return holonyms | set(synset.part_holonyms())"
      ],
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9IuyC5kJ-Q1"
      },
      "source": [
        "Retrieve the synset of a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcpH2HepJ-Q1"
      },
      "source": [
        "syn = wordnet.synsets('muffler')[0]"
      ],
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpUzKmifJ-Q1"
      },
      "source": [
        "Use the function to retrieve all holonyms of the word in a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnJxcIwjJ-Q1",
        "outputId": "d4c96a94-f1ac-45af-a466-f9f98a7130cf"
      },
      "source": [
        "get_holonyms(syn)"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Synset('automobile_engine.n.01'), Synset('car.n.01'), Synset('exhaust.n.02')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVWD_zqbFKGH"
      },
      "source": [
        "## 5. Entailment "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6nCx1dzK6M5"
      },
      "source": [
        "Define function to get the entailments of a synset using union of sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ-5hRegK6NA"
      },
      "source": [
        "def get_entailments(synset):\r\n",
        "  entailments = set()\r\n",
        "  for entailment in synset.entailments():\r\n",
        "    entailments |= set(get_entailments(entailment))\r\n",
        "  return entailments | set(synset.entailments())"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JLMCusNK6NA"
      },
      "source": [
        "Retrieve the synset of a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WmoQi4qK6NA",
        "outputId": "7a857165-9270-420e-ebcb-d0965f560302"
      },
      "source": [
        "syn = wordnet.synsets('buy')[1] # Using [1] this time for the verb synset\r\n",
        "print(syn)"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Synset('buy.v.01')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2cEfT5JK6NB"
      },
      "source": [
        "Use the function to retrieve the entailments of the word in a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUSOCpj5K6NB",
        "outputId": "cc3f81c9-f4b7-4bd6-b9cf-fd3cd5973c27"
      },
      "source": [
        "get_entailments(syn)"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Synset('choose.v.01'), Synset('pay.v.01')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 271
        }
      ]
    }
  ]
}